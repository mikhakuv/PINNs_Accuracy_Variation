# Теория  
В данной работе применяется метод PINN для решения нелинейного дифференциального уравнения второго порядка, а полученный результат сравнивается с аналитическим решением. Исследуется влияние параметров аналитического решения на точность аппроксимации.
### Уравнение
Рассматривается обобщённое уравнение Шрёдингера в нелинейной среде (generalized Schrodinger equation with a dual-power law nonlinear medium):
$$iq_t + q_{xx} + |q|^2 q (1 - \alpha |q|^2 + \beta |q|^4) = 0$$
Аналитическое решение такого уравнения при $\alpha = 0, \beta = 0$ известно и получается из решения, найденного в статье [[1]](#обзор-литературы):
$$q(x, t)=\frac{(4 k^{2} - 4 w)}{e^{\sqrt{k^{2} - w} (- 2 k t + x - x_0)} + 2(k^{2}-w)e^{-\sqrt{k^{2} - w} (- 2 k t + x - x_0)}} e^{i(k x - w t + \theta_{0})},$$
рассматривается  случай $x_0 = 0, \theta_{0} = 0$, при этом $k$ и $w$ варьируются.  
### PINN
Решение уравнения находится в виде нейросети. Это возможно потому, что нейросеть рассматривается как функция, а от функции можно считать производные разных порядков и из них составить исходное уравнение. Полученное уравнение будет использоваться как первое слагаемое `loss` (далее обозначается как `loss_f`). Задачей оптимизатора будет как можно сильнее уменьшить `loss`, а значит и заставить нейросеть удовлетворять уравнению. Чтобы в итоге не получалось тривиальное решение, вторым слагаемым `loss` будет ошибка выполнения начальных и граничных условий (далее обозначается как `loss_uv`). В данной работе начальное условие определяется как значения аналитического решения в $(x,t_0)$, а граничные условия считаются нулевыми в $(x_0,t)$ и $(x_1,t)$. В итоге минимизируется следующая функция: `loss = loss_f + loss_uv`. Весь процесс изображён на картинке:  
<img src="https://github.com/mikhakuv/PINNs_for_article/blob/main/pictures/illustration.png">  
Такой подход называется **P**hysics **I**nformed **N**eural **N**etwork и был впервые представлен в [[2]](#обзор-литературы). Он существенно отличается от численных методов тем, что в итоге получается не массив чисел, а дифференцируемая функция.  
### Теорема аппроксимации
Тот факт, что с помощью нейросети можно сколь угодно близко приблизить заданную функцию теоретически обоснован в статье [[3]](#обзор-литературы). Для нашего случая актуальна теорема 2.2(Theorem 2.2), которая накладывает минимальные условия на функцию активации: непрерывность и непостоянность. Тем не менее, теорема доказана для однослойной сети. Для многослойной же сети можно использовать заключение 2.6(Corollary 2.6).
Согласно этому заключению, нейросети с несколькими слоями могут выступать универсальными аппроксиматорами векторнозначных функций. Но при этом на функцию активации накладываются условия: она должна быть *сжимающей* (в оригинале squashing) или такой, что однослойная сеть из этих функций приближает *сжимающую* функцию на компакте. Под *сжимающей* функцией имеется в виду такая $\psi: \mathbb{R} \rightarrow [0,1]$, что: 
# Обзор Литературы  
1. !!!статья с уравнением
2. *Maziar Raissi, Paris Perdikaris, George Em Karniadakis* "Physics Informed Deep Learning (Part I): Data-driven Solutions of Nonlinear Partial Differential Equations"
3. *K. Hornik, M. Stinchcombe, H. White* "Multilayer feedforward networks are universal approximators"
