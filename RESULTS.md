# Теория  
В данной работе применяется метод PINN для решения нелинейного дифференциального уравнения второго порядка, а полученный результат сравнивается с аналитическим решением. Исследуется влияние параметров аналитического решения на точность аппроксимации.
### Уравнение
Рассматривается обобщённое уравнение Шрёдингера в нелинейной среде (generalized Schrodinger equation with a dual-power law nonlinear medium):
$$iq_t + q_{xx} + |q|^2 q (1 - \alpha |q|^2 + \beta |q|^4) = 0$$
Аналитическое решение такого уравнения при $\alpha = 0, \beta = 0$ известно и получается из решения, найденного в статье [[1]](#обзор-литературы):
$$q(x, t)=\frac{(4 k^{2} - 4 w)}{e^{\sqrt{k^{2} - w} (- 2 k t + x - x_0)} + 2(k^{2}-w)e^{-\sqrt{k^{2} - w} (- 2 k t + x - x_0)}} e^{i(k x - w t + \theta_{0})},$$
рассматривается  случай $x_0 = 0, \theta_{0} = 0$, при этом $k$ и $w$ варьируются.  
### PINN
Решение уравнения находится в виде нейросети. Это возможно потому, что нейросеть рассматривается как функция, а от функции можно считать производные разных порядков и из них составить исходное уравнение. Полученное уравнение будет использоваться как первое слагаемое `loss` (далее обозначается как `loss_f`). Задачей оптимизатора будет как можно сильнее уменьшить `loss`, а значит и заставить нейросеть удовлетворять уравнению. Чтобы в итоге не получалось тривиальное решение, вторым слагаемым `loss` будет ошибка выполнения начальных и граничных условий (далее обозначается как `loss_uv`). В данной работе начальное условие определяется как значения аналитического решения в $(x,t_0)$, а граничные условия считаются нулевыми в $(x_0,t)$ и $(x_1,t)$. В итоге минимизируется следующая функция: `loss = loss_f + loss_uv`. Весь процесс изображён на картинке:  
<img src="https://github.com/mikhakuv/PINNs_for_article/blob/main/pictures/illustration.png">  
Такой подход называется **P**hysics **I**nformed **N**eural **N**etwork и был впервые представлен в [[2]](#обзор-литературы). Он существенно отличается от численных методов тем, что в итоге получается не массив чисел, а дифференцируемая функция.  
### Теорема аппроксимации
Тот факт, что с помощью нейросети можно сколь угодно близко приблизить заданную функцию теоретически обоснован в статье [[3]](#обзор-литературы). Для нашего случая актуальна теорема 2.2(Theorem 2.2), которая накладывает минимальные условия на функцию активации: непрерывность и непостоянность. Тем не менее, теорема доказана для однослойной сети. Для многослойной же сети можно использовать заключение 2.6(Corollary 2.6).
Согласно этому заключению, нейросети с несколькими слоями могут выступать универсальными аппроксиматорами векторнозначных функций. При этом на функцию активации накладываются условия: она должна быть *сжимающей* (в оригинале squashing) или такой, что однослойная сеть из этих функций приближает *сжимающую* функцию на компакте. Под *сжимающей* функцией имеется в виду такая $\psi: \mathbb{R} \rightarrow [0,1]$, что:  
1) $\psi$ возрастающая(non-decreasing)
2) $\lim\limits_{\lambda\to -\infty} \psi(\lambda) = 0$
3) $\lim\limits_{\lambda\to \infty} \psi(\lambda) = 1$

(Ясно, что используемая в данной работе функция активации $sin(x)$ удовлетворяет условию заключения 2.6. Достаточно вспомнить теорему Вейерштрасса о приближении функций тригонометрическими многочленами и учесть тот факт, что однослойная сеть с функцией активации $sin(x)$ является таким тригонометрическим многочленом при определённых значениях подбираемых коэффициентов.)(это чисто моё рассуждение, можно не включать если не уверены) 
Таким образом в данной работе нейросеть понимается как аппроксиматор, способный приближать решение рассматриваемого дифференциального уравнения.
# Методика измерений и результаты  
### Параметры нейросети
В опытах использовалась нейросеть с топологией [2,100,100,100,100,2]: 2 входа - переменные $x$ и $t$; дальше идут 4 полносвязных слоя по 100 нейронов в каждом; 2 выхода - действительная($u$) и мнимая($v$) части. Обучение происходит за 30000 итераций оптимизатора Adam с темпом обучения, затухающим по закону `lr=0,99*lr` каждые 100 итераций. Функция активации нейронов - sin. Используемые параметры получены методом перебора как наиболее подходящие.
### Методика вычисления ошибок
Мерой успеха считалось значение функции $mse_q$ , вычисляемое как: $$mse_q=\frac{\sum\limits_{i=1}^n(\sqrt{u_{truth}(x_i,t_i)^2 +v_{truth}(x_i,t_i)^2} - \sqrt{u_{pred}(x_i,t_i)^2 +v_{pred}(x_i,t_i)^2})^2}{n}$$ где $n$ - количество рассматриваемых точек на области  
Также вычислялись $mse_u$, $mse_v$, $mse_{f_u}$, $mse_{f_v}$ и $rar$(Relative Amplitude Reduction):
$$mse_u = \frac{\sum\limits_{i=1}^n(u_{truth}(x_i,t_i)-u_{pred}(x_i,t_i))^2}{n}$$
$$mse_v = \frac{\sum\limits_{i=1}^n(v_{truth}(x_i,t_i)-v_{pred}(x_i,t_i))^2}{n}$$
$$mse_{f_u} = \frac{\sum\limits_{i=1}^n(f_{pred_u}(x_i,t_i))^2}{n}$$
$$mse_{f_v} = \frac{\sum\limits_{i=1}^n(f_{pred_v}(x_i,t_i))^2}{n}$$
$$rar = \frac{\max\limits_{x \in [x_0,x_1]} (\sqrt{u_{truth}(x,t_1)^2+v_{truth}(x,t_1)^2}) - \max\limits_{x \in [x_0,x_1]} (\sqrt{u_{pred}(x,t_1)^2+v_{pred}(x,t_1)^2})}{\max\limits_{x \in [x_0,x_1]} (\sqrt{u_{truth}(x,t_1)^2+v_{truth}(x,t_1)^2})}$$
### Результаты
Опыты проводились на области $x \in [-10;30]$, $t \in [0;3,5]$ для разных значений $k$ и $w$. Был исследован каждый случай из приведённой ниже таблицы:  
|       | w=1,7     | w=2,1     | w=2,5     |
|-------|-----------|-----------|-----------|
| k=2,0 | exp(1, 1) | exp(1, 2) | exp(1, 3) |
| k=2,5 | exp(2, 1) | exp(2, 2) | exp(2, 3) |
| k=3,0 | exp(3, 1) | exp(3, 2) | exp(3, 3) |  

(код в латехе: \begin{table}[!ht]
    \centering
    \begin{tabular}{|l|l|l|l|}
    \hline
        ~ & w=1,7 & w=2,1 & w=2,5 \\ \hline
        k=2,0 & exp(1, 1) & exp(1, 2) & exp(1, 3) \\ \hline
        k=2,5 & exp(2, 1) & exp(2, 2) & exp(2, 3) \\ \hline
        k=3,0 & exp(3, 1) & exp(3, 2) & exp(3, 3) \\ \hline
    \end{tabular}
\end{table})  
В ячейках находятся названия соответствующих опытов в таблице [performance_table(av).xlsx](https://github.com/mikhakuv/PINNs_Accuracy_Variation/blob/main/statistics/performance_table(av).xlsx)  
На графике 1 изображена зависимость точности решения $mse_q$ от параметра $w$, разные кривые соответствуют разным значениям $k$:  

На приведённом выше графике видно, что:  
**1.** При увеличении $k$ точность решения существенно снижается  
**2.** При увеличении $w$ точность решения немного увеличивается  

Похожие результаты наблюдаются и для остальных метрик: $mse_u$, $mse_v$, $mse_{f_u}$, $mse_{f_v}$ и $rar$. Чем можно объяснить такую зависимость? Вспомним формулу аналитического решения:
# Обзор Литературы  
1. !!!статья с уравнением
2. *Maziar Raissi, Paris Perdikaris, George Em Karniadakis* "Physics Informed Deep Learning (Part I): Data-driven Solutions of Nonlinear Partial Differential Equations"
3. *K. Hornik, M. Stinchcombe, H. White* "Multilayer feedforward networks are universal approximators"
