# Теория  
В данной работе применяется метод PINN для решения нелинейного дифференциального уравнения второго порядка, а полученный результат сравнивается с аналитическим решением. Исследуется влияние параметров аналитического решения на точность аппроксимации.
### Уравнение
Рассматривается обобщённое уравнение Шрёдингера в нелинейной среде (generalized Schrodinger equation with a dual-power law nonlinear medium):
$$iq_t + q_{xx} + |q|^2 q (1 - \alpha |q|^2 + \beta |q|^4) = 0$$
Аналитическое решение такого уравнения при $\alpha = 0, \beta = 0$ известно и получается из решения, найденного в статье [[1]](#обзор-литературы):
$$q(x, t)=\frac{4(k^{2} - w)}{e^{\sqrt{k^{2} - w} (- 2 k t + x - x_0)} + 2(k^{2}-w)e^{-\sqrt{k^{2} - w} (- 2 k t + x - x_0)}} e^{i(k x - w t + \theta_{0})},$$
рассматривается  случай $x_0 = 0, \theta_{0} = 0$, при этом $k$ и $w$ варьируются.  
### PINN
Решение уравнения находится в виде нейросети. Это возможно потому, что нейросеть рассматривается как функция, а от функции можно считать производные разных порядков и из них составить исходное уравнение. Полученное уравнение будет использоваться как первое слагаемое `loss` (далее обозначается как `loss_f`). Задачей оптимизатора будет как можно сильнее уменьшить `loss`, а значит и заставить нейросеть удовлетворять уравнению. Чтобы в итоге не получалось тривиальное решение, вторым слагаемым `loss` будет ошибка выполнения начальных и граничных условий (далее обозначается как `loss_uv`). В данной работе начальное условие определяется как значения аналитического решения в $(x,t_0)$, а граничные условия считаются нулевыми в $(x_0,t)$ и $(x_1,t)$. В итоге минимизируется следующая функция: `loss = loss_f + loss_uv`. Весь процесс изображён на картинке:  
<img src="https://github.com/mikhakuv/PINNs_Accuracy_Variation/blob/main/pictures/illustration.png">
Такой подход называется **P**hysics **I**nformed **N**eural **N**etwork и был впервые представлен в [[2]](#обзор-литературы). Он существенно отличается от численных методов тем, что в итоге получается не массив чисел, а дифференцируемая функция.  
### Теорема аппроксимации
Тот факт, что с помощью нейросети можно сколь угодно близко приблизить заданную функцию теоретически обоснован в статье [[3]](#обзор-литературы). Для нашего случая актуальна теорема 2.2(Theorem 2.2), которая накладывает минимальные условия на функцию активации: непрерывность и непостоянность. Тем не менее, теорема доказана для однослойной сети. Для многослойной же сети можно использовать заключение 2.6(Corollary 2.6).
Согласно этому заключению, нейросети с несколькими слоями могут выступать универсальными аппроксиматорами векторнозначных функций. При этом на функцию активации накладываются условия: она должна быть *сжимающей* (в оригинале squashing) или такой, что однослойная сеть из этих функций приближает *сжимающую* функцию на компакте. Под *сжимающей* функцией имеется в виду такая $\psi: \mathbb{R} \rightarrow [0,1]$, что:  
1) $\psi$ возрастающая(non-decreasing)
2) $\lim\limits_{\lambda\to -\infty} \psi(\lambda) = 0$
3) $\lim\limits_{\lambda\to \infty} \psi(\lambda) = 1$

(Ясно, что используемая в данной работе функция активации $sin(x)$ удовлетворяет условию заключения 2.6. Достаточно вспомнить теорему Вейерштрасса о приближении функций тригонометрическими многочленами и учесть тот факт, что однослойная сеть с функцией активации $sin(x)$ является таким тригонометрическим многочленом при определённых значениях подбираемых коэффициентов.)(это чисто моё рассуждение, можно не включать если не уверены) 
Таким образом в данной работе нейросеть понимается как аппроксиматор, способный приближать решение рассматриваемого дифференциального уравнения.
# Методика измерений и результаты  
### Параметры нейросети
В опытах использовалась нейросеть с топологией [2,100,100,100,100,2]: 2 входа - переменные $x$ и $t$; дальше идут 4 полносвязных слоя по 100 нейронов в каждом; 2 выхода - действительная($u$) и мнимая($v$) части. Обучение происходит за 30000 итераций оптимизатора Adam с темпом обучения, затухающим по закону `lr=0,99*lr` каждые 100 итераций. Функция активации нейронов - sin. Используемые параметры получены методом перебора как наиболее подходящие.
### Методика вычисления ошибок
Мерой успеха считалось значение функции $mse_q$ , вычисляемое как: $$mse_q=\frac{\sum\limits_{i=1}^n(\sqrt{u_{truth}(x_i,t_i)^2 +v_{truth}(x_i,t_i)^2} - \sqrt{u_{pred}(x_i,t_i)^2 +v_{pred}(x_i,t_i)^2})^2}{n}$$ где $n$ - количество рассматриваемых точек на области  
Также вычислялись $mse_u$, $mse_v$, $mse_{f_u}$, $mse_{f_v}$ и $rar$(Relative Amplitude Reduction):
$$mse_u = \frac{\sum\limits_{i=1}^n(u_{truth}(x_i,t_i)-u_{pred}(x_i,t_i))^2}{n}$$
$$mse_v = \frac{\sum\limits_{i=1}^n(v_{truth}(x_i,t_i)-v_{pred}(x_i,t_i))^2}{n}$$
$$mse_{f_u} = \frac{\sum\limits_{i=1}^n(f_{pred_u}(x_i,t_i))^2}{n}$$
$$mse_{f_v} = \frac{\sum\limits_{i=1}^n(f_{pred_v}(x_i,t_i))^2}{n}$$
$$rar = \frac{\max\limits_{x \in [x_0,x_1]} (\sqrt{u_{truth}(x,t_1)^2+v_{truth}(x,t_1)^2}) - \max\limits_{x \in [x_0,x_1]} (\sqrt{u_{pred}(x,t_1)^2+v_{pred}(x,t_1)^2})}{\max\limits_{x \in [x_0,x_1]} (\sqrt{u_{truth}(x,t_1)^2+v_{truth}(x,t_1)^2})}$$
### Результаты
Опыты проводились на области $x \in [-10;30]$, $t \in [0;3,5]$ для разных значений $k$ и $w$. Был исследован каждый случай из приведённой ниже таблицы:  
|       | w=1,7     | w=2,1     | w=2,5     |
|-------|-----------|-----------|-----------|
| k=2,0 | exp(1, 1) | exp(1, 2) | exp(1, 3) |
| k=2,5 | exp(2, 1) | exp(2, 2) | exp(2, 3) |
| k=3,0 | exp(3, 1) | exp(3, 2) | exp(3, 3) |  

<!---код в латехе:
\begin{table}[!ht]
    \centering
    \begin{tabular}{|l|l|l|l|}
    \hline
        ~ & w=1,7 & w=2,1 & w=2,5 \\ \hline
        k=2,0 & exp(1, 1) & exp(1, 2) & exp(1, 3) \\ \hline
        k=2,5 & exp(2, 1) & exp(2, 2) & exp(2, 3) \\ \hline
        k=3,0 & exp(3, 1) & exp(3, 2) & exp(3, 3) \\ \hline
    \end{tabular}
\end{table})
-->
В ячейках находятся названия соответствующих опытов из таблицы [performance_table(av).xlsx](https://github.com/mikhakuv/PINNs_Accuracy_Variation/blob/main/statistics/performance_table(av).xlsx)  
На графике 1 изображена зависимость точности решения $mse_q$ от параметра $k$, разные кривые соответствуют разным значениям $w$:  

<p align="center"><img src="https://github.com/mikhakuv/PINNs_Accuracy_Variation/blob/main/pictures/results_chart1.PNG"><br><caption>График 1</caption></p>  

На приведённом выше графике видно, что:  
**1.** При увеличении $k$ точность решения существенно снижается  
**2.** При увеличении $w$ точность решения немного увеличивается  

Похожие результаты наблюдаются и для остальных метрик: $mse_u$, $mse_v$, $mse_{f_u}$, $mse_{f_v}$ и $rar$. На графике 2 изображена зависимость амплитуды $|q|$ от $t$ для разных $k$ и $w$:

<p align="center"><img src="https://github.com/mikhakuv/PINNs_Accuracy_Variation/blob/main/pictures/results_chart2.PNG"><br><caption>График 2</caption></p>  

Хорошим является результат, при котором кривые предсказанного и аналитического решений не отличимы. Видно, что увеличение $k$ приводит к сильной разнице в амплитудах аналитического и полученного решений, а увеличение $w$ немного уменьшает эту разницу. Это означает, что $rar$ ведёт себя так же, как и $mse_q$ при изменениях коэффициентов $k$ и $w$.  
Чтобы объяснить такую зависимость, вспомним формулу аналитического решения: $$q(x, t)=\frac{4(k^{2} - w)}{e^{\sqrt{k^{2} - w} (- 2 k t + x - x_0)} + 2(k^{2}-w)e^{-\sqrt{k^{2} - w} (- 2 k t + x - x_0)}} e^{i(k x - w t + \theta_{0})}$$
Обратим внимание на выражение $k^2-w$, которое обозначим как $\mu$. Видно, что от $\mu$ зависит амплитуда решения, а от $\sqrt{\mu}$ зависят показатели экспонент, определяющие как сильно будет меняться их значение при изменении $x$ и $t$. Поэтому при увеличении $\mu$ амплитуда решения будет увеличиваться, как и скорость изменения относительно $x$ и $t$.
На графиках 3 и 4 показано, как меняется решение при изменении $\mu$:

<p align="center"><img src="https://github.com/mikhakuv/PINNs_Accuracy_Variation/blob/main/pictures/results_chart3.PNG"><br><caption>График 3</caption></p>  

<p align="center"><img src="https://github.com/mikhakuv/PINNs_Accuracy_Variation/blob/main/pictures/results_chart4.PNG"><br><caption>График 4</caption></p>  

Предположения подтверждаются, видно как при увеличении $\mu$ возрастает амплитуда и уменьшается ширина пиков. Это делает функцию более сложной для аппроксимации и ошибка $mse_q$ возрастает, что и видно на графике 5, отображающем зависимость $mse_q(\mu)$:  

<p align="center"><img src="https://github.com/mikhakuv/PINNs_Accuracy_Variation/blob/main/pictures/results_chart5.PNG"><br><caption>График 5</caption></p>  

При этом зависимость $\mu$ от $k$ квадратичная с положительным знаком, а от $w$ линейная с отрицательным знаком, что и проясняет результаты, полученные на графике 1: при увеличении $k$ существенно увеличивается $\mu$, а значит существенно уменьшается точность; при увеличении $w$ немного уменьшается $\mu$, а точность наоборот растёт.  
Тем не менее, $k$ и $w$ встречаются в выражении не только в составе $\mu$, но и по отдельности, что усложняет эффекты, возникающие при при их изменении и зависимость $mse_q$ от них не является линейной.  


Также при росте $\mu$ можно наблюдать увеличение слагаемых решаемого уравнения. Как уже упомянуто выше, для нахождения комплексного решения у нейросети есть 2 выхода: действительная($u$) и мнимая($v$) части. Именно от них по отдельности берутся производные, из которых потом составляются действительная($Re(F)$) и мнимая($Im(F)$) части уравнения - выражения, которые нужно как можно лучше приблизить к 0. Они получаются из исходного уравнения при подстановке $q(x,t)=u(x,t)+i v(x,t)$:  
$Re(F(q,q_{t},q_{xx}))=-v_t+u_xx+u*(u^2+v^2)(1-\alpha(u^2+v^2)+\beta(u^2+v^2)^2)$  
$Im(F(q,q_{t},q_{xx}))=u_t+v_xx+v*(u^2+v^2)(1-\alpha(u^2+v^2)+\beta(u^2+v^2)^2)$  
Этот момент уточнён для того, чтобы обосновать изучение производных действительной и мнимой частей функции по отдельности, ведь при оптимизации они находятся отдельно. На графике 6 видно, как увеличиваются все слагаемые обоих минимизируемых выражений $Re(F)$ и $Im(F)$ при росте $\mu$ и фиксированном $t=0$:  

<p align="center"><img src="https://github.com/mikhakuv/PINNs_Accuracy_Variation/blob/main/pictures/results_chart6.PNG"><br><caption>График 6</caption></p>  

В приведённой ниже таблице рассчитано, во сколько раз увеличивается максимальное значение каждого из слагаемых при $t=0$(при других $t$ картина похожая):  
|             | $\mu$=2.30 | $\mu$=4.15 | $\mu$=6.50 | relation |
|-------------|---------|---------|---------|----------|
| u*(u^2+v^2) | 7.074   | 13.046  | 19.815  | 2.801    |
| v*(u^2+v^2) | 8.909   | 23.282  | 46.570  | 5.228    |
| u_t         | 6.878   | 14.254  | 24.843  | 3.612    |
| v_t         | 6.989   | 14.439  | 24.464  | 3.501    |
| u_xx        | 12.086  | 24.756  | 43.000  | 3.558    |
| v_xx        | 13.000  | 29.672  | 55.428  | 4.264    |

<!---код в латехе:
\begin{table}[!ht]
    \centering
    \begin{tabular}{|l|l|l|l|l|}
    \hline
        ~ & mu=2.30 & mu=4.15 & mu=6.50 & relation \\ \hline
        u*(u\^2+v\^2) & 7.074 & 13.046 & 19.815 & 2.801 \\ \hline
        v*(u\^2+v\^2) & 8.909 & 23.282 & 46.570 & 5.228 \\ \hline
        u\_t & 6.878 & 14.254 & 24.843 & 3.612 \\ \hline
        v\_t & 6.989 & 14.439 & 24.464 & 3.501 \\ \hline
        u\_xx & 12.086 & 24.756 & 43.000 & 3.558 \\ \hline
        v\_xx & 13.000 & 29.672 & 55.428 & 4.264 \\ \hline
    \end{tabular}
\end{table}
-->  
Видно, что наибольший рост испытывает слагаемое $v*(u^2+v^2)(1-\alpha(u^2+v^2)+\beta(u^2+v^2)^2)$, которое при используемых в данной работе значениях коэффициентов принимает вид: $v*(u^2+v^2)$. Рост более чем пятикратный, остальные слагаемые тоже заметно увеличиваются. Вероятно это также осложняет процесс обучения и точность решения опять снижается.  

Статистика по всем проведённым экспериментам и данные для построения графиков находятся в файлах:
[performance_table(av).xlsx](https://github.com/mikhakuv/PINNs_Accuracy_Variation/blob/main/statistics/performance_table(av).xlsx),
[amplitude_statistics.xlsx](https://github.com/mikhakuv/PINNs_Accuracy_Variation/blob/main/statistics/amplitude_statistics.xlsx)  
### Вывод
В данной работе изучалась зависимость точности получаемого с помощью PINN решения от коэффициентов $k$ и $w$ в начальном условии. Было получено, что при увеличении $k$ и уменьшении $w$ решение становится сложнее аппроксимировать, а слагаемые минимизируемых выражений растут, из-за чего точность решения ожидаемо снижается.  
# Обзор Литературы  
1. !!!статья с уравнением
2. *Maziar Raissi, Paris Perdikaris, George Em Karniadakis* "Physics Informed Deep Learning (Part I): Data-driven Solutions of Nonlinear Partial Differential Equations"
3. *K. Hornik, M. Stinchcombe, H. White* "Multilayer feedforward networks are universal approximators"
